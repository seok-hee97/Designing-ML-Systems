# CH11 머신러닝의 인간적 측면(The Human Side of Machine Learning)   

- ML 시스템은 기술뿐 아니라 비즈니스 의사 결정자, 사용자, 시스템 개발자를 포함함
## 11.1. 사용자 경험
- ML 시스템은 결정론적이 아니라 확률론적임
  : 일반적으로 전통적인 소프트웨어 시스템의 경우 동일한 소프트웨어에서 동일한 입력 데이터로    
  각기 다른 시간에 두 번 실행하면 동일한 결과를 얻을 것이라 기대하지만,    
  ML 시스템의 경우 같은 입력 데이터로 각기 다른 시간이 두 번 실행하면 다른 결과를 얻을 수 있음.    

- 이러한 확률론적 특성 때문에 ML 시스템 예측은 대부분 맞지만, 보통 ML 시스템이 어떤 입력에 대해 맞을 줄 몰라,   
  이것이 어려운 부분임
  ML 시스템의 규모가 커지는 경우에는 예측을 생성하는 데 예상외로 오래 걸림
  => ML 시스템이 사용자 경험에 기존과 다른 방식으로 영향을 미치고 있음

- 사용자가 전통적인 소프트웨어에 익숙하다면 더욱 그런데, ML이 도입된 기간이 상대적으로 짧아     
  ML 시스템이 사용자 경험에 어떤 영향을 미치는지 아직 충분히 연구되지 않았음      
 
#### <사용자 경험 일관성 보장하기>  
- 사용자는 앱이나 웹사이트를 사용할 때 일정 수준의 일관성을 기대함   
  : 예를 들어 크롬 사용시 '최소화' 버튼은 맥북 좌측 상단 모서리에 있는데,    
  이 위치가 오른쪽으로 옮겨지면 혼란스러울 수 있음

- ML 예측은 확률론적이며 일관적이지 않음     
  예측 컨텍스트에 따라 오늘 어떤 사용자에게 생성된 예측이 다음 날 같은 사용자에 대해 생성되는 예측과 다름   
  => ML을 활용해 사용자 경험을 개선하려는 작업에서는 일관적이지 않은 ML 예측이 방해가 될 수 있음   

- 2020년 부킹닷컴에서는 숙박 시설 예약시, 사용자가 세션에서 사용한 필터를 기반으로 원하는 필터를 자동으로 제안하자고 했으나    
  난제로 'ML 모델이 매번 다른 필터를 제안하면 사용자가 혼란스러워 할 수 있다' 였음      
  이 난제를 시스템이 동일한 필터 추천 사항을 반환해야 하는 조건(예 : 사용자가 필터를 적용한 경우) 와    
  시스템이 신규 추천 사항을 반환하는 조건 (예: 사용자가 목적지를 변경한 경우)을 지정하는 규칙을 만들었음     

  이를 일관성-정확도 트레이드오프(consistency-accuracy trade-off) 라고 하는데,    
  시스템에서 가장 정확하다고 간주하는 추천 사항이 사용자에게 일관성을 제공하는 추천 사항이 아닐 수도 있음     

#### <'대부분 맞는' 예측에 맞서기>     
- 2018년 부터 대형 언어모델 GPT 이후, 후속 모델이 나오면서,    
  대규모 언어 모델의 장점은 작업별 훈련 데이터가 거의 혹은 전혀 필요하지 않은 상태에서 광범위한 작업에 대한 예측을 생성한다는 점임.    

- 하지만 이 모델의 단점의 예측이 항상 옳지는 않아(대부분 맞지만),    
  예측을 개선하기 위해 작업 별 데이터를 미세 조정하는 비용이 높다는 점임.   

- 대부분 맞는 예측은 예측 결과를 쉽게 볼 수 있는 사용자에게 유용함.     
  : 고객 지원 ML 시스템은 각 고객 요청에 대부분 맞는 응답을 생성해    
  인간 운영자는 해당 응답을 신속하게 편집할 수 있음.

- 다만 대부분 맞는 예측은 사용자가 응답을 수정하는 방법을 모르거나    
  수정할 수 없다면 그다지 유용하지 않음

=> 이를 극복하기 위한 접근법은 동일한 입력에 대한 여러 예측 결과를 사용자에게 표시해 적어도 하나 이상이 맞을 가능성을 높이는 것이고,    
이 예측 결과들이 비전문가 사용자도 평가할 수 있는 방식으로 렌더링돼야 한다는 점임     

사용자가 입력한 일련의 요구사항이 주어질 때 모델은 리액트 코드 스니펫을 여러 개 생성하고, 코드 스니펫은 시각적 웹페이지로 렌더링되고,    
엔지니어가 아닌 사용자가 자신에게 가장 적합한 것을 평가할 수 있음     

=> 이 접근법은 매우 일반적으로 ***휴먼 인 더 루프(human-in-the-loop AI)*** 라고 함.     
인간이 개입해 최상의 예측을 선택하거나 기계가 생성한 예측을 개선함      

#### <원만한 실패>     
- ML 모델 추론 대기 시간이 사용자 경험에 미치는 영향이 있기 때문에,   
  추론 속도를 높이기 위해 모델을 압축하고 최적화 하는 방법이 있음     
- 그러나 빠른 모델도 특정 쿼리에는 시간이 걸리는데, 언어 모델이나 시계열 모델 같이 순차적 데이터를 처리하는 모델에서는    
  짧은 시리즈보다 긴 시리즈를 처리하는데 오래 걸림     
   
=> 일부 회사들은 '백업 시스템' 을 사용하는데, 주 시스템에 비해 최적은 아니지만 빠른 예측이 가능함    
휴리스틱이나 단순 모델일수 있으며, 미리 계산된 예측을 캐싱할 수 있음    

- 주 모델이 예측을 생성하는데 X밀리초보다 오래 걸리면 대신 백업 모델을 사용하세요' 와 같은 규칙을 지정해서,    
  주 모델이 주어진 쿼리에 대한 예측을 생성하는데 걸리는 시간을 예측하고 해당 예측을 주 모델 혹은 백업 모델에 적절히 라우팅함     
  => 추가된 모델로 인해 ML 시스템 추론 레이턴시가 증가함     

- 속도-정확도 트레이드오프가 있긴 하는데, 어떤 모델은 다른 모델보다 성능이 낮지만 추론을    
  훨씬 더 빨리 수행해서 최적은 아니지만 빠른 모델은 사용자에게 좀 덜 좋은 예측을 제공하지만 레이턴시가 중요한 사황에서는 선호됨     

- 두 모델 중 하나를 선택해야 하지만, 백업 시스템을 사용하면 추론 속도가 빠른 모델과 정확도가 높은 모델을 모두 사용할 수 있음    

#### 11.2. 팀 구조    
- ML 프로젝트에는 데이터 과학자와 ML 엔지니어뿐 아니라 데브옵스 엔지니어나 플랫폼 엔지니어처럼    
  다른 유형의 엔지니어와 주제 전문가(SME) 같은 비개발자 이해관계자도 포함됨     

- 다양한 이해관계자가 있을 때 최적의 ML 팀 구조는 (1) Cross-functional(크로스-펑셔널) 팀의 협업과    
  (2) end-to-end 엔드-투-엔드 데이터 과학자의 역할임     

#### 크로스-펑셔널(cross-functional) 팀 협업     
- ML 시스템 설계시 많은 시스템이 해당 주제에 대한 전문 지식 없이는 작동하지 않음.    
  SME는 ML 시스템의 사용자이면서 개발자임
- 대부분 주제 전문 지식을 데이터 레이블링 단계에서만 생각하지만,   
  ML 모델 훈련 프로세스가 프로덕션에서도 이어서 진행됨에 따라 레이블링과 재레이블링 프로세스    
  또한 전체 프로젝트 수명 주기에 걸쳐 진행됨   
- 나머지 수명 주기에 SME를 포함시키면 많은 이점을 얻을 수 있는데,    
  문제 정의 및 공식화/피처 엔지니어링/오류 분석/ 모델 평가/리랭킹 예측 및 사용자 인터페이스에 SME가 큰 도움이 됨    
- SME가 프로젝트 계획 단계 초기에 참여하도록 하고, 엔지니어들에게 권한 부여를 요청하지 않고도 프로젝트에 기여할 수 있어야 함     
  많은 기업에서 코드를 작성하지 않고도 변경 가능한 노코드(no-code), 로우코드(low-code) 플랫폼을 구축해    
  SME가 ML 시스템 개발에 더 많이 참여하도록 돕고 있음    
- 현재 SME를 위한 노코드 ML 솔루션은 대부분 레이블링, 품질 보증, 피드백 단계에 있지만    
  더 많은 플랫폼이 데이터셋 생성 및 SME 의 입력이 필요한 문제를 조사하기 위한 뷰 등을 지원하기 위해 개발되고 있음     

##### 엔드-투-엔드 데이터 과학자     
- ML 프로덕션은 ML 문제일 뿐 아니라 인프라의 문제이기도 했는데, MLOps를 수행하려면    
  ML 전문 지식뿐 아니라 배포, 컨테이너화, 작업 오케스트레이션 및 워크플로 관리와 관련된 운영 전문 지식이 필요함     
- 기업에서는 이런 전문성을 ML 프로젝트에 적용하기 위해 (1) 모든 운영 측면을 관리하는 팀을 별도로 두거나,    
  (2) 팀에 데이터 과학자를 포함시켜 전체 프로세스를 담당하게 함

(1) 별도의 팀을 구성해 프로덕션 관리     
- 데이터 과학 및 ML 팀은 개발 환경에서 모델을 개발하고,    
  별도의 팀(운영/플랫폼/ML 엔지니어링 팀)이 프로덕션 환경에 모델을 배포함

- 이 접근은 인력고용이 용이한고 개개인이 한 가지에만 집중하면 되지만 단점이 존재함

  단점 [1] 커뮤니케이션 및 조정 오버 헤드    

  - 한 팀이 다른 팀에 방해가 될 수 있음      
  단점 [2] 디버깅 난제    

  - 무언가 실패했을 때 어느 팀의 코드가 원인인지 알 수 없음     
    무엇이 잘못됐는지 파악하기 위해 여러 팀의 협력 필요   
단점 [3] 책임 미루기

  - 문제가 무엇인지 파악한 후에도 각 팀은 이를 수정하는 책임이 다른 팀에 있다고 생각할 수 있음    
단점 [4] 좁은 맥락

  - 어느 누구도 전체 프로세스를 개선하는 가시성을 가지고 있지 않음     
  - 플랫폼 팀은 인프라 개선 방법에 대한 아이디어가 있지만, 데이터 과학자의 요청이 있을 때만 대응함.    
    데이터 과학자는 인프라를 다루지 않아도 되어 인프라를 선제적으로 변경할 이유가 없어짐   

(2) 데이터 과학자가 전체 프로세스를 담당하도록 함    

- 이 접근법 사용 시 데이터 과학 팀은 모델의 프로덕션 적용 또한 고려해야 함      
  - 데이터 과학자는 프로세스에 모든 것을 알고 있어야 해서, 데이터 과학보다 상용 코드를 더 작성하게 될 수도 있음     

- 저수준 인프라를 알게될수록 데이터 과학자가 이를 알기를 기대하기 비합리적일 수 있는데,    
  인프라에 필요한 기술은 데이터 과학과는 매우 다르기 때문임    

- 데이터 과학자가 전체 프로세스를 담당하려면 좋은 도구가 필요하고,    
  좋은 인프라가 필요하기 때문에 인프라 걱정 없이 프로세스를 엔드-투-엔드로 담당할 수 있는 추상화가 필요함     

    Ex, 도구에    
    '여기 데이터를 저장하는 곳(S3), 코드를 실행하는 단계(피처링, 모델링),    
    여기서 코드를 실행해야하고(EC2 인스턴스, AWS 배치, AWS 람다와 같은 서버리스 서비스),    
    각 단계에서 코드를 실행하는데 필요한 것들(종속성, dependencies)' 라고 이 도구가 모든 인프라를 관리한다면 ?    

    => 풀스택 데이터 과학자의 성공은 어떤 도구를 갖췄는지에 달려있음.    
    '컨테이너화, 분산 처리, 자동 장애 조치 및 기타 고급 컴퓨터 과학 개념의 복잡성으로 부터 데이터 과학자를 추상화' 하는 도구가 필요함.    

## 11.3. 책임 있는 AI   
- 지능형 시스템을 책임 있게 만드는 방법에 대한 문제는 ML 시스템뿐 아니라 일반 인공 지능(AI) 시스템과도 관련 있음.   
- 책임 있는 AI(responsible AI)는 사용자에게 권한을 부여하고, 신뢰를 낳고,    
  사회에 공정하고 긍정적인 영향을 보장하기 위해 좋은 의도와 충분한 인식으로 AI 시스템을 설계, 개발, 배포하는 관행임.    
- 공정성, 개인 정보 보호, 투명성, 책임과 같은 영역으로 구성됨.     
- ML 시스템 개발자는 시스템이 사용자와 사회 전반에 어떤 영향을 미칠지 고려해야 하고,    
  시스템에 윤리 및 안전 포괄성을 구체적으로 구현해 모든 이해관계자가 사용자에 대한 책임을 인식하도록 도울 책임이 있음.    

#### <무책임한 AI : 사례 연구>  
[ 사례 연구 1 : 자동 채점기 편향 ]  

- 영국이 코로19로 인해 대학 배치를 결정하는 중요한 시험 A-레벨을 취소하고,    
  시험을 치르지 않고 학생들에게 최종 A-레벨 성적을 할당하는 자동화 시스템을 사용했음.     

- 교사 평가에 근거에 학생에게 점수를 부여하는 방식을 거부해서,    
  특정 통계 모델 '알고리즘'을 사용해 이전 성적 데이터와 교사 평가를 결합해 성적을 할당하는 편이 공정하다고 추측했음.    

- 그러나, 이 알고리즘에 따른 결과가 부당하고 신뢰할 수 없는 것으로 밝혀졌는데,    
  2019년 데이터로 테스트한 모델이 A-레벨 응시자들에 대해 60%의 평균 정확도를 보였기 때문임.     
  모델이 배정한 성적 중 40%는 학생들의 실제 성적과 다를 것으로 예상된 것임.    

- 모델 정확도가 낮지만 알고리즘 정확도가 인간 채점자의 정확도와 대체로 비슷하다고 옹호했는데,    
  채점관의 점수를 선임 채점관이 작성한 점수와 비교했을 때 약 60%만 일치했고,    
  인간 채점관과 알고리즘의 낮은 정확도가 단일 시점에서 학생을 평가할 때 근본적인 불확실성이 있음을 보여줘 불만을 부추겼음.    

- 위의 자동 채점 시스템은  
  (1) 올바른 목표를 설정하지 못함  
  (2) 잠재 편향을 발견하기 위한 세분화된 평가를 수행하지 못함  
  (3) 모델을 투명하게 만들지 못함  
  이라는 세 가지 실패가 있었음.  

  실패 (1) 잘못된 목표 설정
  - 학생들에게 점수를 매기는 자동 채점 시스템을 개발할 때 시스템 목표는 '채점 정확도' 였을 것임  
  - 그러나, 위에서 최적화 하는 것처럼 모이는 목표는 학교 간의 '기준 유지' 였음. 
  즉, 모델이 예측한 성적을 각 학교의 과거 성적 분포에 맞추는 것임  
  Ex, A 학교의 과거 입시 실적이 B 학교를 능가했다면, 평균적으로 A 학교 학생들에게 B 학교 학생들보다 더 높은 점수를 주는 알고리즘을 원했고,   
  학생 간의 공정성보다 학교 간의 공정성을 우선시했음.   
  즉, 개인의 성적을 올바르게 매기는 모델보다 학교 수준의 결과를 올바르게 매기는 모델을 선호했음.    

  이 목표에 따라 모델이 과거 입시 실적이 저조한 학교에서 성적이 우수한 집단을 불균형적으로 강등했음.    

=> 자원이 많은 학교가 자원이 적은 학교를 능가하는 경향이 있다는 사실을 고려하지 않았고,      
알고리즘 자동 채점기는 학생의 현재 성적보다 학교의 과거 입시 실적을 우선시함으로써,    
소외 계층 학생이 많이 재학하는 자원 부족 학교의 학생들에게 불이익을 줌.    

실패 (2) 편향을 발견하는 세분화된 모델 평가 부족.     
- 과거 입시 실적이 저조한 학교의 학생들에 대한 편향은 이 모델에서 발견된 많은 편향 중 하나일 뿐으로, 
  자동 채점 시스템은 교사의 평가를 입력으로 고려했지만, 인구통계학적 그룹에서 교사의 평가 불일치를 해결하지는 못함.    
- 모델이 각 학교의 과거 입시 실적을 고려해서, 모델에 소규모 학교에 대한 데이터가 충분하지 않았고,     
  소규모 학교의 경우 최종 등급을 매기는 대신 교사가 평가한 등급만 사용하였음.   
  => 인원이 적은 경향이 있는 사립 학교의 학생들에게 더 나은 등급을 줌.    
- 모델이 예측한 성적을 공개하고 다양한 데이터 샘플로 세분화된 평가를 수행했다면 편향을 발견할 수 있었을 것임.    
  EX, 다양한 규모의 학교와 다양한 배경을 가진 학생들에 대한 모델 정확도를 평가하는 것!    

실패 (3) 투명성 부족

- 투명성은 시스템에 대한 신뢰를 구축하는 첫 단계임.   
- 하지만 이 알고리즘 자동 채점기의 중요한 측면은 너무 늦기전에 공개됐음.    
  시스템 목적이 학교 간의 공정성을 유지하는 것임을 성적이 발표되는 날 까지 알리지 않았고,    
  대중들이 모델이 개발되는 동안 이 목표에 대한 우려를 표할 수 없었음.    

- 고려 사항은 좋은 의도에서 나왔지만, 시스템이 독립적인 외부 조사를 충분히 받지 못했음.     
  
=> 이 사례 연구는 많은 사람들의 삶에 직접적인 영향을 미치는 모델을 구축할 때 투명성이 얼마나 중요한지 보여주며,     
모델의 중요한 측면을 적시에 공개하지 않으면 어떤 결과를 얻게 되는지 알 수 있음.    
최적화할 목표를 올바르게 선택하는 일의 중요성 또한 보여주며, 잘못된 목표(학교 간의 공정성 우선시)를 설정하면    
올바른 목표에 대해서는 성능이 저조한 모델을 선택하게 될 뿐 아니라 편견을 영속화 할 수 있음.   

- 이 사례 연구를 통해 알고리즘으로 무엇을 자동화하고, 무엇을 자동화하지 말아야 하는지에 대한 사이의 모호한 경계를 보여줌.     
경계가 명확해질 때까지는 AI 알고리즘을 오용하는 사례가 많아지기 때문에, 경계가 명확해지도록 시간과 자원을 더 많이 투자해야 함.    

[ 사례 연구 2 : 익명화된 데이터의 위험성 ].    

- 민감한 데이터가 유출되도록 하는 요인은 데이터 인터페이스와 데이터 수집 설계 방식임.     
- ML 시스템 개발은 데이터 품질에 크게 의존하므로 사용자 데이터를 수집하는 일이 중요함.    
- 연구 커뮤니티는 새로운 기술을 개발하기 위해 고품질 데이터셋에 엑세스하고,    
  실무자와 기업은 새로운 유스 케이스를 발견하고 새로운 AI 기반 제품을 개발하기 위해 데이터에 액세스 해야함   
- 데이터셋을 수집하고 공유함으로써 데이터셋의 일부인 사용자의 개인 정보와 보안이 침해되기도 하는데,    
  사용자 보호를 위해 개인 식별 정보(PII, Personally Identifiable Information)을 익명화 해야 한다는 주장이 제기되기도 함.    
- 그러나 익명화는 개인 정보를 보호하고 데이터 오용을 막는데 충분하지 않음.    
- 데이터 수집 및 공유는 AI 같은 데이터 기반 기술 개발에 필수지만 잠재적인 위험 또한 존재함.   
- 데이터를 익명화하고 선의로 공개한 경우에도 잠재 위험이 있음을 보여줌.   

#### 책임 있는 AI의 프레임 워크.   
(1) 모델 편향의 출처 찾아내기

- ML 시스템 설계에 관한 논의에서 편향이 워크플로 전체에서 발생한다는 점을 알게됨.    

- 이러한 편향이 어떻게 반영되는지 알아내야 하는데, 데이터 소스의 예시이며 이 목록 외에 많은 소스가 있음.     

- 편향을 해결하기 어려운 이유 중 하나는 편향이 프로젝트 수명 주기 내 어느 단계에서 발생할 수 있기 때문임.     

  [1] 훈련 데이터

- 모델 개발에 사용한 데이터가 모델이 실제로 처리할 데이터를 대표하는가?     
  그렇지 않으면 모델은 훈련 데이터에 표시되는 데이터가 적은 사용자 그룹에 대해 편향적이 될 수 있음.   

  [2] 레이블링

- 인간 어노테이터(human annotator)가 데이터를 레이블링한다면 레이블 품질을 어떻게 측정할까?    
  어노테이터가 레이블링할 때 주관적인 경험이 아닌 표준 지침을 따르게 하려면?   
  이노테이터가 주관적 경험에 의존할수록 인간의 편향이 개입할 여지가 커짐.   

[3] 피처 엔지니어링

- 모델이 민감한 정보가 포함된 피처를 사용하는지?     
  모델이 일부 집단 사람들에게 이질적인 영향을 끼치는지?    
  이질적인 영향은 선택 프로세스가 중립적인 것처럼 보여도 서로 다른 그룹 간에 매우 다른 결과를 가져올 때 발생함.     
  모델의 결정이 민족/성별/종교 등 법적으로 보호되는 계층과 상관관계가 있는 정보에 의존할 때,     
  이 정보가 모델을 직접 훈련하는데 사용되지 않더라도 발생할 수 있음.     
  Ex, 채용 과정에서 우편 번호, 고등학교 졸업장처럼 인종과 상관관계가 있는 변수를 활용하면 인종별로 이질적인 영향을 미치게 됨.      

[4] 모델의 목표.    

- 모든 사용자에게 공평하게 적용될 수 있는 목표로 모델을 최적화하고 있는지?    

- 모델 성능을 우선시해서 대사수 사용자 그룹에 대해 모델을 왜곡하고 있지는 않은지?      

[5] 평가

- 다양한 사용자 그룹에 대한 모델 성능을 이해하기 위해 적절하고 세분화된 평가를 수행하고 있는지?     

- 공정하고 적절한 평가는 공정하고 적절한 평가 데이터가 존재하는지에 달려 있음.    

(2) 데이터 기반 접근법의 한계 이해하기

- ML은 데이터를 기반으로 문제를 해결하기 위한 접근법임.    
- 하지만 데이터로는 충분하지 않는데, 데이터느 실제 사람들에 관한 거싱며 고려해야 할 사회경제적/문화적 측면이 있음.    

- 데이터에 과하게 의존함으로써 생기는 맹점을 잘 이해해야 하는데, 구축하는 ML 시스템에 영향을 받을 사람들의 실제 경험을    
  녹여낼 수 있도록 현업의 도메인 전문가와 논의해 도메인 지식(규울, 기능 등)을 파악해야 함    

=> 공정한 자동 채점 시스템 구축을 위해 반드시 도메인 전문가와 협력해 학생들의 인류통계학적 분포와    
사회경제적 요인이 과거 성적 데이터에 어떻게 반영되는지 이해해야 함

(3) 서로 다른 요구 사항 간의 트레이드오프 이해하기

- ML 시스템 구축 시 시스템이 특정 속성을 갖추기를 원하게 되는데,    
  낮은 추론 레이턴시를 원한다면 가지치기 같은 모델 압축 기술로 달성할 수 있음     
  높은 예측 정확도를 원한다면 더 많은 데이터를 추가함으로써 달성할 수 있음    
- 모델이 공정하고 투명하기를 원한다면 공개 조사를 위해 모델과 해당 모델을 개발하는데    
  사용한 데이터 액세스가 가능하도록 해야할 수 있음
- 종종 ML 문헌에서 한 속성에 대한 최적화가 나머지 속성을 정적으로 유지한다는 비현실적인 가정을 하는데,    
  사람들은 특정 모델의 정확도나 레이턴시가 그대로 유지된다는 가정하에 모델의 공정성을 개선하기 위한 기술을 논의함     
  => 그러나, 실제로는 한 속성을 개선하면 한 속성이 저하될 수 있음

##### <대표적인 트레이드오프 두 가지 예>    

[1] 개인 정보 보호와 정확도 간의 트레이드오프      

- 위키백과에 다르면 차등 개인 정보 보호는 데이터셋 내 개인에 대한 정보는 숨기면서    
  그룹들의 패턴을 설명함으로써 데이터셋에 대한 정보를 공개적으로 공유하는 시스템임.    
  차등 개인 정보 보호가 담고 있는 개념은 데이터베이스에서 임의 단일 대체의 효과가 충분히 작다면,     
  쿼리 결과로 개인에 대해 많은 것을 추론할 수 없다는 점에서 개인 정보를 보호한다는 점임    

- 차등 개인 정보 보호는 ML 모델의 훈련 데이터에 흔히 사용되는 기술인데,     
  이 때 트레이드오프는 차등 개인 정보 보호가 제공하는 개인 정보 보호 수준이 높을 수록 모델의 정확도가 낮아진다는 점임     
  그러나, 정확도 감소는 모든 샘플에서 동일하지는 않음     
- 차등 개인 정보 보호 모델의 정확도는 과소 대표된(underrepresented)된 클래스와 하위 그룹에서 훨씬 크게 감소한다는 연구가 있음     

[2] 간결함과 공정성 간의 트레이드오프

- 정확도 손실을 최소한으로 억제하면서도 모델 매개변수를 90% 줄일 수 잇는데,    
  최소한으로 억제된 손실이 모든 클래스에 균일하게 분산돼 있다면?
  소수의 클래스에만 정확도 손실이 집중된다면?    

- 한 연구에서는 서로 다른 개수의 가중치를 가진 모델이 비슷한 최상위 성능 지표를 갖지만,    
  데이터셋 샘플 데이터 집합에서는 추론 결과가 매우 다르다는 것을 발견함     
  Ex, 성별/인종/장애 등 보호된 피처가 데이터 분포의 롱테일 클래스 일때,       
  저빈도 클래스 일때 압축 기술이 정확도 손실을 증폭함.   
  이는 압축이 제대로 표현되지 않은 피처에 불균형적으로 영향을 미침을 의미함.   

- 연구에서 얻은 또다른 중요한 발견은 평가한 압축 기법이 모두 불균일한 영향을 미치지만,    
  모든 기법이 동일한 수준으로 영향을 미치지 않는 다는 점으로, 가지치기는 양자화 기법보다 더 이질적인 영향을 미침.     

- 유사한 트레이드오프가 계속 발견되고 있는데 ML 시스템을 설계할 때 정보에 입각한 결정을 내리려면 이러한 트레이드오프를 인식하는 것이 중요함.    

(4) 사전 대응하기

- ML 시스템 개발 주기가 빨라질수록 ML 시스템이 사용자의 삶에 어떤 영향을 미치는지, 어떤 편향을 가지는지 생각해야 함.   
  => 이러한 편향은 미리 해결하는 편이 비용이 낮음

(5) 모델 카드 생성하기
 
- 모델 카드는 훈련된 ML 모델과 함께 제공되는 짧은 문서로,  
  모델이 어떻게 훈련되고 평가됐는 지에 대한 정보를 제공함.  
- 모델 카드는 모델이 사용되는 컨텍스트와 제한 사항 또한 공개하고,    
  모델 카드의 목표는 이해관계자가 배포를 윟나 후보 모델을 비교할 때 전통적인 평가 지표뿐 아니라     
  윤리적, 포괄적, 공정한 고려사항의 축을 따르도록 함으로써 윤리적 관행과 보고를 표준화 하는 것.    

##### <모델에 대해 보고 할 수 있는 정보>.    

- 모델 세부 정보 : 모델에 대한 기본 정보
  - 모델을 개발하는 개인 혹은 조직
  - 모델 날짜
  - 모델 버전
  - 모델 유형
  - 훈련 알고리즘, 매개변수, 공정성 제약 및 그 외에 적용된 접근법과 피처에 대한 정보
  - 추가 정보를 위한 논문과 기타 자료
  - 인용 세부 정보
  - 특허
  - 모델에 대한 질문이나 의견을 보낼 곳

- 사용 목적 : 개발 중에 구상한 유스 케이스
  - 주요 용도
  - 주요 사용자
  - 범위 외 유스케이스
- 요인 : 요인에는 인구통계학적 또는 표현형 그룹, 환경 조건, 기술적 속성 등이 포함
  - 관련 요인
  - 평가 요인
- 지표 : 모델의 잠재 영향을 반영하는 지표 선택
  - 모델 성능
  - 결정 임곗값
  - 변형 접근법
- 평가 데이터 : 카드의 정량 분석에 사용한 데이터셋에 대한 세부 정보
  - 데이터 셋
  - 동기
  - 전처리
- 훈련데이터 : 실제로는 훈련 데이터를 제공하지 못할 수 있기에 가능하면, 이 섹션은 평가 데이터를 반영해야함.    
  이러한 세부 정보가 가능하지 않다면 훈련 데이터셋의 다양한 요인에 대한 분포 세부 정보 등 최소한의 정보를 모델 카드에 제공    

- 정량적 분석
  - 단일 결과
  - 교차 결과

- 윤리적 고려 사항

- 주의 사항과 권장 사항

=> 모델 카드는 ML 모델 개발의 투명성을 높여줌    
모델 사용자가 해당 모델을 개발한 사람이 아닐 경우 특히 중요함     
=> 모델이 업데이트 될 때마다 모델 카드를 업데이트 해야 하는데, 자주 업데이트 되는 모델의 경우    
모델 카드를 수동으로 생성하면 데이터 과학자에게 상당한 오버헤드가 발생함    

  - 따라서 모델 카드를 자동으로 생성하는 도구를 보유하는 것이 중요함   
  
(6) 편향 완화를 위한 프로세스 수립하기  

- 책임 있는 AI를 구축하는 프로세스는 복잡해서, 프로세스가 임시방편적일수록 오류가 발생할 여지가 많음  
- 구글의 AI에 대한 권장 모범 사례나 IBM이 보유한 오픈 소스 AIF360  
- IBM의 AIF360은 데이터셋의 모델의 편향을 완화하기 위한 일련의 지표, 설명, 알고리즘을 포함함  
- 그 외에 서드파티 감사를 고려할 수도 있음  
  
(7) 책임 있는 AI에 관한 최신 정보 파악하기   

- AI는 빠르게 움직이는 분야로 새로운 편향이 계속해서 발견되고 있고,     
  책임 있는 AI에 관한 난제도 끊임없이 새로 등장함

## 정리
- ML 솔루션은 기술적이지만 ML 시스템 설계는 기술 영역에 국한되지 않음    
  
- ML 시스템을 개발하고 사용하는 것은 인간이고, 시스템은 사회에 흔적을 남김     
  
- ML 시스템은 확률적이고, 대부분 맞지만 레이턴시가 높은 특성이 사용자 경험에 영향을 미치는데,    
  확률론적 특성으로 인해 사용자 경험에 일관성이 없어져 불만을 야기할 수 있음     

- ML 시스템의 '대부분 맞는' 특성은 사용자가 예측을 쉽게 수정할 수 없는 경우 시스템을 무용지물로 만들기 때문에,    
  동일한 입력에 대해 '가장 맞는' 예측 몇 개를 보여주고 하나는 맞기를 바람   

- ML 시스템을 구축하려면 여러 기술 세트가 필요할 때가 많은데, 다른 기술 세트를 담당하는 여러 팀을 참여시킬 수도 있고    
  데이터 과학 팀에서 모든 기술을 담당하게 할 수도 있음   
  
- 다른 기술 세트를 담당하는 여러 팀을 참여시키면 주요 단점은 커뮤니케이션 오버헤드고     
  두 번째 엔드 투 엔드는 데이터 과학자를 고용하기 어렵다는 점임  

- 책임 있는 AI는 더 이상 단순한 추상화가 아니라 오늘날 ML 산업의 필수 관행으로 빠르게 조치해야 함.    
  윤리 원칙을 모델링 및 조직 관행에 통합하면 전문 데이터 과학자 및 ML 엔지니어로서의 두각을 나타낼 수 있음   