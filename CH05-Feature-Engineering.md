# CH05 Feature Engineering

- 올바른 피처를 보유하는 것은 ML 모델을 개발하는데 중요함
- 실행가능한 모델이 있는 한 올바른 피처를 보유하는 것이 하이퍼파라미터 조정 같은 알고리즘 기법보다 큰 향상을 이끌어낼 수도 있음


## <5.1 학습된 피처 vs 엔지니어링된 피처>

- 딥러닝을 수행하면 피처 엔지니어링이 필요 엔지니어링이 필요 업시도 함.    
  딥러닝의 장점은 피처를 수작업으로 만들 필요가 없어서, 딥러닝의 치퍼 학습이라고도 함     
- 많은 피처들이 알고리즘에 의해 자동으로 학습되고 추출되지만, 모든 피처를 자동화하려면 아직 멀었음   
- 프로덕션용 ML 애플리케인션의 대부분은 딥러닝이 아니기도 함    
  
  for example, 댓글의 스팸 여부를 분류하기 위해 감성 분류기를 구축한다고 가정할 떄, 딥러닝 이전에는   
  텍스트 조각이 주어졌을 때 '표제어 추출(Lemmatization)', '줄임말 확장(expanding contraction)',   
  '구두점(punctuation) 제거', '소문자화(lowercasing)' 와 같은 고전적인 텍스트 처리 기술을   
  수동으로 적용하고,텍스트를 n-gram으로 분할함    


- n-gram은 주어진 텍스트 샘플 내 항목 n개의 연속 시퀀스임    
  항목은 '음소(phoneme)', '음절(syllable)', 문자 또는 단어임   
  예를 들어, 'I like food' 라는 게시물이 주어지면 단어 레벨 1-gram은 ['I', 'like', 'food'],   
  단어 레벨 2-gram은 ['I like', 'like food'] 이다.   
  n이 1과 2 일때 이 문장의 n-gram 피처 집합은 ['I', 'like','food', 'I like', 'like food'] 이다.   

![img](img/fig5-1.png)
  텍스트에 대한 n-gram 피처를 수작업으로 생성하는 텍스트 처리 기법은   
  (1) 원본 텍스트 -> (2) 불용어 제거 -> (3) 표제어 추출 -> (4) 줄임말 확장   
  -> (5) 구두점 제거 -> (6) 소문화 -> (7) 토큰화 -> (8) N-gram   

  훈련 데이터에 대한 n-gram을 생성한 뒤에는 각 n-gram을 인덱스에 매핑하는 어휘(vocabulary)를 생성하여,   
  n-gram의 인덱스 기반으로 벡토로 변환함.    

  n-gram의 어휘가 7이면 각 게시물은 요소 7개로 구성된 벡터가 되며,    
  각 요소는 해당 인덱스의 n-gram이 게시물에 나타나는 횟수에 해당함    

  I->0, like->1, good->2, food->3, I like 4 ->, good food->5, like food->6    
  'I like food'는 벡터 [1,1,0,1,1,0,1] 로 인코딩 됨    

  피처 엔지니어링에는 도메입녈 기술에 대한 지식이 필요함.
  도메인은 자연어 처리이고, 텍스트 언어는 영어임

- 피처 엔지니어링은 프로세스는 반복적인 경향이 있어, 따라서 취약할 수 있음.   

- 위에 있는 자연어처리 시 표제어, 구두점 또는 불용어(stopword) 제거를 신경 쓸 필요 없이 원시 텍스트를 단어로 분할(토큰화: tokenization)하고,    
  단어로 어휘를 만들고 이를 사용개 각 단어를 원-핫 벡터로 변환하여, 모델이 이러부터 유용한 피처를 추출하는 것을 학습함    


- ML 시스템에 텍스트와 이미지 외의 데이터가 필요한 경우가 있는데, 예뜰 들면 댓글의 스팸 여부를 감지 할 때는     
  댓글 텍스트 왜에 댓글의 수(찬성과 반대가 각각 몇 개 인지),  
  댓글을 게시한 사용자(계정이 언제 생성됐고 얼마나 자주 게시하면 찬성과 반대를 얼마나 많이 얻었는지),     
  댓글이 게시된 스레드(조회수가 몇인지, 인기 있는 스레들일수록 스팸성 댓글이 많은 경향이 있음)    


- 피처 엔지니어링은 사용할 정보를 선택하고 이 정보를 ML 모델에서 사용하는 포맷으로 추출하는 프로세스임   


## <5.2 피처 엔지니어링 기법>   

- 데이터의 피처를 전처리 할때 고려할 중요한 작업은   
  결측값(missing value) 처리, 스케일링(scaling), 이산화(discretization),    
  범주형(categorical) 피처 인코딩, 교차(cross) 피처와 위치(positional) 피처 생성 등이 있음     


##### [결측값 처리]

- 결측값에는 세가지 유형
  - (1) 비무작위결측(MNAR, Missing not at random): 결측값이 발생한 이유가 실제 값 자체에 있음   
    for exmaple, 일부 응답자가 소득을 공개하지 않았을 때, 소득을 신고하지 않은 응답자가 소득을 신고한   
    응답자보다 소득이 더 높은 경향이 있다고 판명 될 수 있는 것처럼, 소득 값이 누락된 이유는 '값 자체와 관련됨'  

  - (2) 무작위결측(MAR, Missing at Random): 결측값이 발생한 이유가 값 자체가 아닌 다른 관측 변수에 있음    

    예시에서 성별이 'A'인 응답자가의 연령 값이 누락된 경우가 있는데, 설문 조사에서 성별이 'A'인 응답자가   
    연령 공개를 원하지 않아서 일 수도 있음    


  - (3) 완전 무작위 결측(MCAR, Missing completey at random): 결측값에 패턴이 없음   
    for example, '직업' 열의 결측값은 값이나 다른 변수들 때문이 아니라 완전 무작위임.    
    사람들은 때때로 특별한 이유 없이 값을 채우는 것을 잃어버리는데,  
    이러한 결측은 매우 드물며 보통 값이 누락되는 데는 이유가 있어 조사가 필요함.    

- 결측값은 특정 값으로 채우거나(대치), 제거(삭제)해 처리함    



##### [결측값 삭제(Deletion)]

- 결측값 처리에서 삭제는 '열 삭제'와 '행 삭제' 방법이 있음.  

- 행 삭제는 누락된 값(들)이 있는 샘플을 제거함.  
  : 이 방법은 결측값이 완전 무작위(MCAR) 이며, 결측값 있는 샘플의 비주이 적을 때(예를 들어 0.1%) 유용함.   
  (데이터 샘플의 10%를 제거할 수 없으므로).   

  데이터 행을 제거하면, 결측값이 비무작위(MNAR)의 경우, 모델이 수행하는데  필요한 중요한 정보고 제거될 수 있음    
  데이터 행을 제거하면 결측값이 무작위(MAR)인 경우 모델에 편향이 발생할 수 있음    



##### [결측값 대치(Imputastion)].   

- 데이터 삭제가 더 쉽긴 하지만 중요한 정보가 손실되고 모델에 편향이 발생할 수 있는 단점이 있으므로,   
  결측값을 삭제하는 대신 특정 값으로 채우는 방법이 있음.   

- 결측값을 대치할 때는 일반적으로 기본값으로 채움
  예를 들어, 작업이 누락된 경우 빈 문자열 ''로 채우거나, 결측값을 평균 mean, 중앙값 median,  
  최빈값 mode 로 채우는 일밙거인 방법도 있음    

- 결측값 삭제와 대치 모두 많은 경우에 잘 작동하지만 문제가 있는 사용자 경험을 유발할 수 도 있음

- 일반적으로 결측값을 가능한 값으로 채우지 않는 편이 좋음
  예를 들어 자녀 수 피처의 결측값을 0으로 채우지 않는 것이 좋은데, 값이 0일수도 있기 때문임.   
  결측값을 0으로 채우면 정보가 없는 사람과 자녀가 없는 사람을 구분하기가 어려워짐    

- 특정 데이터셋에 대한 결측값을 처리하기 위해 여러 기법을 동시에 또는 순서대로 사용할 수 있음   
  

- 결측값을 삭제하면 중요한 정보를 잃거나 편향이 강조될 위험이 있고,    
  결측값 대치를 사용하면 데이터에 자신의 편향을 중립하고 데이터에 잡음을 더할 수 있음.    
  심한 경우 데이터 누수 위험이 있음    




##### [스케일링]     

- 예를 들어 12개월 내 주택 구매 여부를 예측하는 작업을 진행할 때, 데이터 연령 변수는 20-40 인 반면,    
  연간 소득 변수는 10,000-150,000 사이임. 두 변수를 ML 모델에 입력하면 모델은 150,000와 40이    
  서로 다른 것을 나타낸 다는 사실을 이해하지 못함    
- 모델이 피처를 입력하기 전에 각 피처를 유사한 범위로 스케일링 하는 것이 중요함 => 피처 스케일링
- 적은 노력으로 모델 성능 향샹을 이끌어 낼 수 있고, 이 작업을 하지 않으면 모델이 이상한 예측을 할 수 있음    
  (그래디언트 부스트 트리나 로지스틱 회구 같은 경우)

- 직관적으로 피처를 스케일링 하려면 [0,1] 범위로 조절함    
  변수 x가 주어졌을 때, 공식적으로 변수 값이 [0,1] 범위에 있도록 조절함    

  







